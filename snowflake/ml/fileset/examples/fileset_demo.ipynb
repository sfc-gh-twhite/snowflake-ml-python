{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fileset library usage guides\n",
    "Snowflake Python FileSet library is one of the Snowflake ML tools. It provides easy approaches to load large data\n",
    "from Snowflake to where you Python code runs. It includes two components: Snowflake Filesystem and Snowflake FileSet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "Snowflake Filesystem/FileSet requires either a\n",
    "[Snowflake Python connection](https://docs.snowflake.com/en/user-guide/python-connector.html) or\n",
    "[Snowpark seesion](https://docs.snowflake.com/en/developer-guide/snowpark/python/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake import connector\n",
    "from snowflake.ml.utils import connection_params\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "# We are using sfctest0 account.\n",
    "connection_parameters = connection_params.SnowflakeLoginOptions(\"sfctest0\")\n",
    "snowpark_session = Session.builder.configs(connection_parameters).create()\n",
    "sf_connection = connector.connect(**connection_parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snowflake Filesystem APIs\n",
    "\n",
    "We provide an implementation of `fsspec.AbstractFileSystem` which enables a filesystem-alike experience for\n",
    "accessing files in an __internal, server-side encrypted__ Snowflake stage.\n",
    "See the Snowflake [documentation](https://docs.snowflake.com/en/sql-reference/sql/create-stage#internal-stage-parameters-internalstageparams)\n",
    "on how to create internal server-side encrypted stages.\n",
    "\n",
    "### Create a new Snowflake Filesystem object\n",
    "The Snowflake Filesystem object can be created by either a Snowflake python connection or Snowpark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "\n",
    "# sfsfs module is required to register \"sfc\" protocol to fsspec.\n",
    "from snowflake.ml.fileset import sfcfs\n",
    "\n",
    "# Create a fs object with snowflake python connection\n",
    "sffs1 = sfcfs.SFFileSystem(sf_connection=sf_connection)\n",
    "# Create a fs object with snowpark session\n",
    "sffs2 = sfcfs.SFFileSystem(snowpark_session=snowpark_session)\n",
    "\n",
    "# Create a fs object with snowflake python connection via fssepc interface\n",
    "sffs3 = fsspec.filesystem(\"sfc\", sf_connection=sf_connection)\n",
    "# Create a fs object with snowpark session via fssepc interface\n",
    "sffs4 = fsspec.filesystem(\"sfc\", snowpark_session=snowpark_session)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some nice feature of fsspec is also inherited by Snowflake Filesystem. For example, you can set the memory cache type and buffer size. You can also utilize the local cache feature of fsspec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_cache_path = \"/tmp/files/\"\n",
    "cached_fs = fsspec.filesystem(\n",
    "    \"filecache\",\n",
    "    target_protocol=\"sfc\",\n",
    "    target_options={\"sf_connection\": sf_connection, \"cache_types\": \"bytes\", \"block_size\": 32 * 2**20},\n",
    "    cache_storage=local_cache_path,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List files in a stage\n",
    "The Snowflake Filesystem can list stage files under a directory in the format of `@<database>.<schema>.<stage>/<filepath>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@ML_DATASETS.public.zhuo_models/zpeng_predict/serving_udf.py',\n",
       " '@ML_DATASETS.public.zhuo_models/zpeng_predict/udf_py_1679730148.zip',\n",
       " '@ML_DATASETS.public.zhuo_models/zpeng_predict/udf_py_1779181627.zip',\n",
       " '@ML_DATASETS.public.zhuo_models/zpeng_predict/udf_py_184582763.zip',\n",
       " '@ML_DATASETS.public.zhuo_models/zpeng_predict/udf_py_261557963.zip',\n",
       " '@ML_DATASETS.public.zhuo_models/zpeng_predict/udf_py_392770850.zip',\n",
       " '@ML_DATASETS.public.zhuo_models/zpeng_predict/udf_py_735600790.zip',\n",
       " '@ML_DATASETS.public.zhuo_models/zpeng_predict/udf_py_73754703.zip']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_fs.ls(\"@ML_DATASETS.public.zhuo_models/zpeng_predict/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': '@ML_DATASETS.public.zzhu_sse/test_data/',\n",
       "  'size': 0,\n",
       "  'type': 'directory',\n",
       "  'md5': None,\n",
       "  'last_modified': None},\n",
       " {'name': '@ML_DATASETS.public.zzhu_sse/ner_dataset.csv.gz',\n",
       "  'size': 3185489,\n",
       "  'type': 'file',\n",
       "  'md5': '61e772df25aea9e527b4238b61dacc0c',\n",
       "  'last_modified': 'Wed, 1 Feb 2023 21:04:48 GMT'},\n",
       " {'name': '@ML_DATASETS.public.zzhu_sse/zzhu_test_cast_0_0_0.snappy.parquet',\n",
       "  'size': 1207,\n",
       "  'type': 'file',\n",
       "  'md5': '50348586e24979d40e448315e8ffa23d',\n",
       "  'last_modified': 'Wed, 11 Jan 2023 21:36:06 GMT'},\n",
       " {'name': '@ML_DATASETS.public.zzhu_sse/zzhu_test_nocast_0_0_0.snappy.parquet',\n",
       "  'size': 1024,\n",
       "  'type': 'file',\n",
       "  'md5': '7c397f20481dd53d514f7438a1fe2f47',\n",
       "  'last_modified': 'Wed, 11 Jan 2023 20:17:25 GMT'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sffs1.ls(\"@ML_DATASETS.public.zzhu_sse\", detail=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and read files\n",
    "You can open a stage file in read mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"PAR1\\x15\\x04\\x15\\xc0\\xae'\\x15\\xa2\\xa2\\x0cL\\x15\"\n"
     ]
    }
   ],
   "source": [
    "with sffs1.open('@ML_DATASETS.public.zzhu_sse/test_data/data_7_7_3.snappy.parquet', mode='rb') as f:\n",
    "    print(f.read(16))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a file can also be done by using fsspec interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"PAR1\\x15\\x04\\x15\\xc0\\xae'\\x15\\xa2\\xa2\\x0cL\\x15\"\n"
     ]
    }
   ],
   "source": [
    "with fsspec.open(\"sfc://@ML_DATASETS.public.zzhu_sse/test_data/data_7_7_3.snappy.parquet\", mode='rb', sf_connection=sf_connection) as f:\n",
    "    print(f.read(16))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to integrate file reading with any plugins that are compatible with fsspec. For example, we can directly read a parquet stage file with pyarrow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "_COL_0: decimal128(38, 0) not null\n",
       "_COL_1: decimal128(38, 0) not null\n",
       "_COL_2: double\n",
       "_COL_3: decimal128(38, 0) not null\n",
       "_COL_4: double\n",
       "_COL_5: double\n",
       "_COL_6: double\n",
       "_COL_7: double\n",
       "_COL_8: double\n",
       "_COL_9: double\n",
       "_COL_10: double\n",
       "_COL_11: double\n",
       "_COL_12: double\n",
       "_COL_13: double\n",
       "_COL_14: double\n",
       "_COL_15: decimal128(38, 0) not null\n",
       "_COL_16: decimal128(38, 0) not null\n",
       "_COL_17: decimal128(38, 0)\n",
       "_COL_18: decimal128(38, 0)\n",
       "_COL_19: decimal128(38, 0) not null\n",
       "_COL_20: decimal128(38, 0)\n",
       "_COL_21: decimal128(38, 0) not null\n",
       "_COL_22: decimal128(38, 0) not null\n",
       "_COL_23: decimal128(38, 0) not null\n",
       "_COL_24: decimal128(38, 0) not null\n",
       "_COL_25: decimal128(38, 0) not null\n",
       "_COL_26: decimal128(38, 0)\n",
       "_COL_27: decimal128(38, 0) not null\n",
       "_COL_28: decimal128(38, 0) not null\n",
       "_COL_29: decimal128(38, 0) not null\n",
       "_COL_30: decimal128(38, 0)\n",
       "_COL_31: decimal128(38, 0) not null\n",
       "_COL_32: decimal128(38, 0) not null\n",
       "_COL_33: decimal128(38, 0)\n",
       "_COL_34: decimal128(38, 0)\n",
       "_COL_35: decimal128(38, 0)\n",
       "_COL_36: decimal128(38, 0)\n",
       "_COL_37: decimal128(38, 0) not null\n",
       "_COL_38: decimal128(38, 0)\n",
       "_COL_39: decimal128(38, 0)\n",
       "_COL_40: decimal128(38, 0)\n",
       "----\n",
       "_COL_0: [[738438,738440]]\n",
       "_COL_1: [[1,0]]\n",
       "_COL_2: [[2,0]]\n",
       "_COL_3: [[-1,1]]\n",
       "_COL_4: [[null,4]]\n",
       "_COL_5: [[null,2]]\n",
       "_COL_6: [[1127,40350]]\n",
       "_COL_7: [[1,null]]\n",
       "_COL_8: [[8,null]]\n",
       "_COL_9: [[0,23]]\n",
       "..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "table = pq.read_table(\"sfc://@ML_DATASETS.public.zzhu_sse/test_data/data_7_7_3.snappy.parquet\", filesystem=sffs1)\n",
    "table.take([1, 3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also read other file formats like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Sentence #,Word,POS,Tag\\r\\n'\n",
      "b'Sentence: 1,Thousands,NNS,O\\r\\n'\n",
      "b',of,IN,O\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "with cached_fs.open(\"sfc://@ML_DATASETS.public.zzhu_sse/ner_dataset.csv.gz\", mode='rb', sf_connection=sf_connection) as f:\n",
    "    f = gzip.GzipFile(fileobj=f)\n",
    "    for i in range(3):\n",
    "        print(f.readline())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With fsspec's native support of local caching, a local copy of the csv file will be created. We can even directly read the local copy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Sentence #,Word,POS,Tag\\r\\n'\n",
      "b'Sentence: 1,Thousands,NNS,O\\r\\n'\n",
      "b',of,IN,O\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "# Check the local cache\n",
    "cache_name = cached_fs.hash_name(\"@ML_DATASETS.public.zzhu_sse/ner_dataset.csv.gz\", False)\n",
    "with open(f'{local_cache_path}{cache_name}', mode='rb') as f:\n",
    "    f = gzip.GzipFile(fileobj=f)\n",
    "    for i in range(3):\n",
    "        print(f.readline())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other supported methods\n",
    "Snowflake filesystem supports most read-only methods supported by fsspec, which includes `find()`, `info()`,\n",
    "`isdir()`, `isfile()`, `exists()`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '@ML_DATASETS.public.zzhu_sse/', 'size': 0, 'type': 'directory'}\n",
      "{'name': '@ML_DATASETS.public.zzhu_sse/ner_dataset.csv.gz', 'size': 3185489, 'type': 'file', 'md5': '61e772df25aea9e527b4238b61dacc0c', 'last_modified': 'Wed, 1 Feb 2023 21:04:48 GMT'}\n"
     ]
    }
   ],
   "source": [
    "print(sffs1.info(\"@ML_DATASETS.public.zzhu_sse/\"))\n",
    "print(sffs1.info(\"@ML_DATASETS.public.zzhu_sse/ner_dataset.csv.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(sffs1.isdir(\"@ML_DATASETS.public.zzhu_sse\"))\n",
    "print(sffs1.isdir(\"@ML_DATASETS.public.zzhu_sse/\"))\n",
    "print(sffs1.isdir(\"@ML_DATASETS.public.zzhu_sse/ner_dataset.csv.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(sffs1.isfile(\"@ML_DATASETS.public.zzhu_sse\"))\n",
    "print(sffs1.isfile(\"@ML_DATASETS.public.zzhu_sse/\"))\n",
    "print(sffs1.isfile(\"@ML_DATASETS.public.zzhu_sse/ner_dataset.csv.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(sffs1.exists(\"@ML_DATASETS.public.zzhu_sse/ner_dataset.csv.gz\"))\n",
    "print(sffs1.exists(\"@ML_DATASETS.public.zzhu_sse/random_name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('@ML_DATASETS.public.zhuo_models', ['00f9c83e76870f14cb5265de20ef8d6cfcc92f6c9e05065b317c6bcd7a479ad7', '184f3054e0dc1085e938ded26d18f47319441ac219866692a5e47fb9ee85ffbe', '36cbf84c124c90cc8c0f122b448d3089167b0f417dd8a2ecf4ef68ccb583b91d', '4cfdd792e91f96b0bd442971b9313877f9dcffde0be71f4009937e5d83d800f8', '8af58850e9d77444ce614c80a18bd051944f869956e24acf54db7863f30753ec', 'a766abf0b3bfc35026401af725d82e2873d50c8c062e62318970ef91eacab670', 'e04cf101c6f55cc320a5ef9a9758ee11fe927ecabda364a2afc49c1d4d3e285a', 'e60557aae588ebb41aa6447a70a528030d68a39b50affc445815b1cf69b8fa83', 'model_too_large', 'zpeng_predict', 'zpeng_predict_separated'], [])\n",
      "('@ML_DATASETS.public.zhuo_models/00f9c83e76870f14cb5265de20ef8d6cfcc92f6c9e05065b317c6bcd7a479ad7', [], ['serving_model.pkl'])\n",
      "('@ML_DATASETS.public.zhuo_models/184f3054e0dc1085e938ded26d18f47319441ac219866692a5e47fb9ee85ffbe', [], ['serving_model_ltitu.pkl'])\n",
      "('@ML_DATASETS.public.zhuo_models/36cbf84c124c90cc8c0f122b448d3089167b0f417dd8a2ecf4ef68ccb583b91d', [], ['serving_model.pkl'])\n",
      "('@ML_DATASETS.public.zhuo_models/4cfdd792e91f96b0bd442971b9313877f9dcffde0be71f4009937e5d83d800f8', [], ['serving_model.pkl'])\n",
      "('@ML_DATASETS.public.zhuo_models/8af58850e9d77444ce614c80a18bd051944f869956e24acf54db7863f30753ec', [], ['serving_model.pkl'])\n",
      "('@ML_DATASETS.public.zhuo_models/a766abf0b3bfc35026401af725d82e2873d50c8c062e62318970ef91eacab670', [], ['serving_model.pkl'])\n",
      "('@ML_DATASETS.public.zhuo_models/e04cf101c6f55cc320a5ef9a9758ee11fe927ecabda364a2afc49c1d4d3e285a', [], ['serving_model_bohkt.pkl'])\n",
      "('@ML_DATASETS.public.zhuo_models/e60557aae588ebb41aa6447a70a528030d68a39b50affc445815b1cf69b8fa83', [], ['serving_model_gaphw.pkl'])\n",
      "('@ML_DATASETS.public.zhuo_models/model_too_large', [], ['udf_py_1428176221.zip', 'udf_py_574940779.zip'])\n",
      "('@ML_DATASETS.public.zhuo_models/zpeng_predict', [], ['serving_udf.py', 'udf_py_1679730148.zip', 'udf_py_1779181627.zip', 'udf_py_184582763.zip', 'udf_py_261557963.zip', 'udf_py_392770850.zip', 'udf_py_735600790.zip', 'udf_py_73754703.zip'])\n",
      "('@ML_DATASETS.public.zhuo_models/zpeng_predict_separated', [], ['serving_udf.py'])\n"
     ]
    }
   ],
   "source": [
    "for i in sffs1.walk(\"@ML_DATASETS.public.zhuo_models\"):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@ML_DATASETS.public.zhuo_models/00f9c83e76870f14cb5265de20ef8d6cfcc92f6c9e05065b317c6bcd7a479ad7/serving_model.pkl',\n",
       " '@ML_DATASETS.public.zhuo_models/184f3054e0dc1085e938ded26d18f47319441ac219866692a5e47fb9ee85ffbe/serving_model_ltitu.pkl',\n",
       " '@ML_DATASETS.public.zhuo_models/36cbf84c124c90cc8c0f122b448d3089167b0f417dd8a2ecf4ef68ccb583b91d/serving_model.pkl',\n",
       " '@ML_DATASETS.public.zhuo_models/4cfdd792e91f96b0bd442971b9313877f9dcffde0be71f4009937e5d83d800f8/serving_model.pkl',\n",
       " '@ML_DATASETS.public.zhuo_models/8af58850e9d77444ce614c80a18bd051944f869956e24acf54db7863f30753ec/serving_model.pkl',\n",
       " '@ML_DATASETS.public.zhuo_models/a766abf0b3bfc35026401af725d82e2873d50c8c062e62318970ef91eacab670/serving_model.pkl',\n",
       " '@ML_DATASETS.public.zhuo_models/e04cf101c6f55cc320a5ef9a9758ee11fe927ecabda364a2afc49c1d4d3e285a/serving_model_bohkt.pkl',\n",
       " '@ML_DATASETS.public.zhuo_models/e60557aae588ebb41aa6447a70a528030d68a39b50affc445815b1cf69b8fa83/serving_model_gaphw.pkl',\n",
       " '@ML_DATASETS.public.zhuo_models/model_too_large/udf_py_1428176221.zip',\n",
       " '@ML_DATASETS.public.zhuo_models/model_too_large/udf_py_574940779.zip',\n",
       " '@ML_DATASETS.public.zhuo_models/zpeng_predict/serving_udf.py',\n",
       " '@ML_DATASETS.public.zhuo_models/zpeng_predict/udf_py_1679730148.zip',\n",
       " '@ML_DATASETS.public.zhuo_models/zpeng_predict/udf_py_1779181627.zip',\n",
       " '@ML_DATASETS.public.zhuo_models/zpeng_predict/udf_py_184582763.zip',\n",
       " '@ML_DATASETS.public.zhuo_models/zpeng_predict/udf_py_261557963.zip',\n",
       " '@ML_DATASETS.public.zhuo_models/zpeng_predict/udf_py_392770850.zip',\n",
       " '@ML_DATASETS.public.zhuo_models/zpeng_predict/udf_py_735600790.zip',\n",
       " '@ML_DATASETS.public.zhuo_models/zpeng_predict/udf_py_73754703.zip',\n",
       " '@ML_DATASETS.public.zhuo_models/zpeng_predict_separated/serving_udf.py']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sffs1.find(\"@ML_DATASETS.public.zhuo_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "828900944"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sffs1.du(\"@ML_DATASETS.public.zhuo_models/zpeng_predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@ML_DATASETS.public.zhuo_models/zpeng_predict/serving_udf.py']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sffs1.glob(\"@ML_DATASETS.public.zhuo_models/zpeng_predict/*_udf*\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snowflake FileSet APIs\n",
    "A Snowflake `FileSet` represents an immutable snapshot of the result of a SQL query in the form of files materialized in\n",
    "an internal server-side encrypted stage. It is built to make user's life easier when do machine learning tasks.\n",
    "\n",
    "### Create a new FileSet object\n",
    "A `FileSet` object can be created with either a Snowflake Python connection, or a Snowpark dataframe. You must also provide\n",
    "an existing internal server-side encrypted stage, or a sub-directory under such a stage for `FileSet` to materialize\n",
    "the data into.\n",
    "See Snowflake [documentation](https://docs.snowflake.com/en/sql-reference/sql/create-stage#internal-stage-parameters-internalstageparams)\n",
    "for how to create such a stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import functions\n",
    "from snowflake.ml.fileset import fileset\n",
    "\n",
    "query = \"SELECT * FROM MYDATA LIMIT 5000000\"\n",
    "\n",
    "# stage \"@ML_DATASETS.public.zhhu_sse\" was created using\n",
    "# CREATE STAGE ML_DATASETS.PUBLIC.ZZHU_SSE ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE');\n",
    "\n",
    "# New FileSet with a Snowpark dataframe\n",
    "df = snowpark_session.sql(query)\n",
    "fileset_init_with_snowpark = fileset.FileSet.make(\n",
    "    target_stage_loc=\"@ML_DATASETS.public.zzhu_sse/\",\n",
    "    name=\"init_with_sp\",\n",
    "    snowpark_dataframe=df,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# New FileSet with a Snowflake Python connection\n",
    "fileset_init_with_conn = fileset.FileSet.make(\n",
    "    target_stage_loc=\"@ML_DATASETS.public.zzhu_sse/\",\n",
    "    name=\"init_with_conn\",\n",
    "    sf_connection=sf_connection,\n",
    "    query=query,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the stage files inside a FileSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_005_0_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_105_0_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_205_0_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_305_0_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_405_0_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_505_0_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_605_0_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_607_0_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_607_0_1.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_705_0_0.snappy.parquet']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileset_init_with_snowpark.files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sfc://@ML_DATASETS.public.zzhu_sse/init_with_conn/data_01aa10fa-0405-a6ee-000c-a90105c77d73_013_5_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_conn/data_01aa10fa-0405-a6ee-000c-a90105c77d73_015_5_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_conn/data_01aa10fa-0405-a6ee-000c-a90105c77d73_015_5_1.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_conn/data_01aa10fa-0405-a6ee-000c-a90105c77d73_113_5_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_conn/data_01aa10fa-0405-a6ee-000c-a90105c77d73_213_5_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_conn/data_01aa10fa-0405-a6ee-000c-a90105c77d73_313_5_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_conn/data_01aa10fa-0405-a6ee-000c-a90105c77d73_413_5_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_conn/data_01aa10fa-0405-a6ee-000c-a90105c77d73_513_5_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_conn/data_01aa10fa-0405-a6ee-000c-a90105c77d73_613_5_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_conn/data_01aa10fa-0405-a6ee-000c-a90105c77d73_713_5_0.snappy.parquet']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileset_init_with_conn.files()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Pytorch\n",
    "Once you created a `FileSet`, you can get a torch `DataPipe` and give it to a torch `DataLoader`. The `DataLoader` iterates the FileSet data and yields batched torch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__NULL_DASK_INDEX__': tensor([377144, 443658, 416818, 413535]), 'LABEL': tensor([1, 0, 1, 0]), 'I1': tensor([nan, nan, 3., 0.]), 'I2': tensor([-1, -1, -1, -1]), 'I3': tensor([nan, nan,  2., 14.]), 'I4': tensor([nan, nan, 0., 6.]), 'I5': tensor([3.3073e+04, 3.0190e+03, 5.0000e+00, 9.7570e+03]), 'I6': tensor([28.,  5.,  0., 55.]), 'I7': tensor([ 0.,  1.,  7., 27.]), 'I8': tensor([ 8.,  0.,  2., 19.]), 'I9': tensor([28.,  5., 62., 29.]), 'I10': tensor([nan, nan, 1., 0.]), 'I11': tensor([0., 1., 3., 3.]), 'I12': tensor([0., nan, nan, nan]), 'I13': tensor([nan, nan, 0., 6.]), 'C1': tensor([  98275684,   98275684, 1761418852,   98275684]), 'C2': tensor([  166103942,  -278296454,   166103942, -1595856491]), 'C3': tensor([ 2107754204, -1951961923,  1332335449,  -732442779]), 'C4': tensor([-2042314393,  -839758040, -1741218962,   144967799]), 'C5': tensor([ 633879704, 1135711049,  633879704, 1135711049]), 'C6': tensor([        nan,  2.1148e+09,  3.2621e+08, -7.2525e+07],\n",
      "       dtype=torch.float64), 'C7': tensor([-180854886, -827473475, -153240071, 1172333472]), 'C8': tensor([185940084, 937732754, 185940084, 185940084]), 'C9': tensor([-1489050352, -1489050352, -1489050352, -1489050352]), 'C10': tensor([ -715045188,   990438539,  1771550005, -1858251738]), 'C11': tensor([ 1524415090,   494662251, -1860483524,  -628503653]), 'C12': tensor([-2119370425,  1003548969,  1865059742, -2043481539]), 'C13': tensor([1533993373,  797164778,  211601238,  374751934]), 'C14': tensor([  131152527, -1299940874,   131152527,   450684655]), 'C15': tensor([  913448412, -1876291425,   913448412,  2026261021]), 'C16': tensor([ 2102101737,  -433704388, -1174001924,  1053666807]), 'C17': tensor([ 881205885, -725910568, -440764814, -440764814]), 'C18': tensor([1525511222, 1271698505, 1525511222,  370602325]), 'C19': tensor([-1357625916,  1679685694,  1739918957,   568184265]), 'C20': tensor([-1537676717, -1537676717, -1322964323,  1480633834]), 'C21': tensor([2046925165, 1586520420,  315350810, 1683039572]), 'C22': tensor([nan, nan, nan, nan], dtype=torch.float64), 'C23': tensor([851920782, 974593739, 974593739, 851920782]), 'C24': tensor([1146932076, 1071331371, -435912013, 1422465304]), 'C25': tensor([-390581241, -390581241, -390581241, -390581241]), 'C26': tensor([ 795788255, 1238795398, 1039539773, -413361431])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dp = fileset_init_with_snowpark.to_torch_datapipe(batch_size=4, shuffle=True, drop_last_batch=True)\n",
    "for batch in DataLoader(dp, batch_size=None, num_workers=0):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed TensorFlow\n",
    "\n",
    "Similarily, you can get a `tf.data.Dataset` from a `FileSet`. Again, the `Dataset` dispenses batched TF Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__NULL_DASK_INDEX__': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([396667, 396684, 391010, 436069])>, 'LABEL': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 0, 0, 0])>, 'I1': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.,  6., nan,  0.], dtype=float32)>, 'I2': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([48,  3, -1,  1])>, 'I3': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([15., nan, nan,  1.], dtype=float32)>, 'I4': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([10.,  7., nan, nan], dtype=float32)>, 'I5': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([  444.,  1085., 33006.,  2617.], dtype=float32)>, 'I6': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([10., 43., 39.,  9.], dtype=float32)>, 'I7': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1., 14.,  1.,  3.], dtype=float32)>, 'I8': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([15., 10.,  7.,  2.], dtype=float32)>, 'I9': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 10., 200.,  25.,   7.], dtype=float32)>, 'I10': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.,  1., nan,  0.], dtype=float32)>, 'I11': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 2., 0., 2.], dtype=float32)>, 'I12': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([nan, nan, nan, nan], dtype=float32)>, 'I13': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([10., 11., nan, nan], dtype=float32)>, 'C1': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 1761418852,    98275684,    98275684, -1017263743])>, 'C2': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([  950618017,   950618017, -1483308162, -1004895032])>, 'C3': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([-2062363315,   574295574, -1354171640,  1901306354])>, 'C4': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1882307430, -900397540, -473155814, 1973176878])>, 'C5': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([633879704, 633879704, 633879704, 633879704])>, 'C6': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([2114768079,  -72524650,  -26504475,  -72524650])>, 'C7': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ -502153781,  -741166262,   954965236, -1469012150])>, 'C8': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 185940084, 1530472565,  185940084, 1530472565])>, 'C9': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([-1489050352, -1489050352, -1489050352, -1489050352])>, 'C10': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 990438539, -753121448, 1417416702,  990438539])>, 'C11': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([-1944528688,  -200466840,  2140143191,   391670744])>, 'C12': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ -501861551,   359635439,  -946961311, -1505179034])>, 'C13': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([-1835498924,  1823810618,  1190406755,  1285550239])>, 'C14': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([-1299940874, -1299940874,   131152527,  1690912869])>, 'C15': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ -385543259,  1122246034,   598321005, -1485657886])>, 'C16': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([-1091627306,  1606371972,  -992060486, -1621693649])>, 'C17': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([-725910568, -440764814,  130367684, -725910568])>, 'C18': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 475380585,  429505257, -994412117,  726041146])>, 'C19': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 0, 0, 0])>, 'C20': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 0, 0, 0])>, 'C21': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1178982188, -558276515, 2052668227, 1262950050])>, 'C22': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([          0,  2028135305, -1389337877,           0])>, 'C23': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([-1099152972,   851920782,   974593739,  1111468905])>, 'C24': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([-1302810776,  -465033377, -1882821000,  1493927279])>, 'C25': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 0, 0, 0])>, 'C26': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 0, 0, 0])>}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "ds = fileset_init_with_snowpark.to_tf_dataset(batch_size=4, shuffle=True, drop_last_batch=True)\n",
    "assert(isinstance(ds, tf.data.Dataset))\n",
    "for batch in ds:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete FileSet\n",
    "You can explicitly call `delete()` to delete the FileSet and its underlying stage. If it is not called, the stage will\n",
    "be preseved, and you can recover it with the path to that stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "fileset_init_with_conn.delete()\n",
    "\n",
    "# The stage should get deleted in the stage.\n",
    "print(sffs1.exists(\"@ML_DATASETS.public.zzhu_sse/init_with_conn\"))\n",
    "\n",
    "# The state of the FileSet will also be cleaned up\n",
    "print(fileset_init_with_conn.files())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recover existing data\n",
    "If an old FileSet is not deleted, you can recover it in another Python program with its stage path and name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_005_0_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_105_0_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_205_0_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_305_0_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_405_0_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_505_0_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_605_0_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_607_0_0.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_607_0_1.snappy.parquet',\n",
       " 'sfc://@ML_DATASETS.public.zzhu_sse/init_with_sp/data_01aa10fa-0405-a6ee-000c-a90105c77d3f_705_0_0.snappy.parquet']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The FileSet recovering will check if all belonging files are generated with the same query id\n",
    "recovered_fileset = fileset.FileSet(\n",
    "    snowpark_session=snowpark_session,\n",
    "    target_stage_loc=\"@ML_DATASETS.public.zzhu_sse\",\n",
    "    name=\"init_with_sp\")\n",
    "\n",
    "recovered_fileset.files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@ML_DATASETS.public.zzhu_sse/test_data/',\n",
       " '@ML_DATASETS.public.zzhu_sse/ner_dataset.csv.gz',\n",
       " '@ML_DATASETS.public.zzhu_sse/zzhu_test_cast_0_0_0.snappy.parquet',\n",
       " '@ML_DATASETS.public.zzhu_sse/zzhu_test_nocast_0_0_0.snappy.parquet']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recovered_fileset.delete()\n",
    "\n",
    "# Now both FileSets are deleted\n",
    "sffs1.ls(\"@ML_DATASETS.public.zzhu_sse/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6443e50212a27c37e6930bf034789243cf2ed3024f210c21f3f3b2d9247b30c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
