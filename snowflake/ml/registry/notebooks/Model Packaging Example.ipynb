{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5de3eb26",
   "metadata": {},
   "source": [
    "# Model Packaging Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "197efd00",
   "metadata": {},
   "source": [
    "## Before Everything"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ce97b36",
   "metadata": {},
   "source": [
    "### Install `snowflake-ml-python` locally"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1117c596",
   "metadata": {},
   "source": [
    "Before `snowflake-ml-python` is publicly available, you have to install from wheel file. Once it is ready, you could install them like other packages in PIP or conda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da314158",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install snowflake_ml_python-0.3.2-py3-none-any.whl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "285c1b29",
   "metadata": {},
   "source": [
    "Notice: It is suggested to use pure-pip environment or empty conda environment when you try this. If you insist to install snowML in a conda environment with packages, it is suggested that you should install all requirements and install `snowflake-ml-python` with `--no-deps` flag."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1b950fe",
   "metadata": {},
   "source": [
    "If you are about to go over the **Use with customize model** part in this notebook, you will need tensorflow and transformers, which could be installed by following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c9fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install snowflake_ml_python-0.3.2-py3-none-any.whl[tensorflow] transformers==4.24.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99e58d8c",
   "metadata": {},
   "source": [
    "### Setup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd16ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale cell width with the browser window to accommodate .show() commands for wider tables.\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ac32c6f",
   "metadata": {},
   "source": [
    "### Start Snowpark Session\n",
    "\n",
    "To avoid exposing credentials in Github, we use a small utility `SnowflakeLoginOptions`. It allows you to score your default credentials in `~/.snowsql/config` in the following format:\n",
    "```\n",
    "[connections]\n",
    "accountname = <string>   # Account identifier to connect to Snowflake.\n",
    "username = <string>      # User name in the account. Optional.\n",
    "password = <string>      # User password. Optional.\n",
    "dbname = <string>        # Default database. Optional.\n",
    "schemaname = <string>    # Default schema. Optional.\n",
    "warehousename = <string> # Default warehouse. Optional.\n",
    "#rolename = <string>      # Default role. Optional.\n",
    "#authenticator = <string> # Authenticator: 'snowflake', 'externalbrowser', etc\n",
    "```\n",
    "Please follow [this](https://docs.snowflake.com/en/user-guide/snowsql-start.html#configuring-default-connection-settings) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2efc0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "session = Session.builder.configs(SnowflakeLoginOptions()).create()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2fcbe4a",
   "metadata": {},
   "source": [
    "### Let `snowflake-ml-python` available for your models to be deployed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "671a7710",
   "metadata": {},
   "source": [
    "Unfortunately, since `snowflake-ml-python` does not exist in Anaconda channel yet, we have to import them manually to use it when the model get deployed to Snowflake. To avoid upload them again and again, we could set up a temporary stage and upload the wheel file there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNOW_ML_WHEEL_LOCAL_PATH = \"~/snowml/bazel-bin/snowflake/ml/snowflake_ml_python-0.3.3-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcececa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "def upload_snowml_to_tmp_stage(session: Session, wheel_path: str, stage_name: Optional[str] = None) -> str:\n",
    "    \"\"\"Upload model module of snowml to tmp stage.\n",
    "\n",
    "    Args:\n",
    "        session: Snowpark session.\n",
    "        wheel_path: Path to the local SnowML wheel file.\n",
    "\n",
    "    Returns:\n",
    "        The stage path to uploaded snowml.zip file.\n",
    "    \"\"\"\n",
    "    if stage_name is None:\n",
    "        stage_name = session.get_session_stage()\n",
    "    _ = session.file.put(wheel_path, stage_name, auto_compress=False, overwrite=True)\n",
    "    whl_filename = os.path.basename(wheel_path)\n",
    "    return f\"{stage_name}/{whl_filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ea99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNOW_ML_WHEEL_STAGE_PATH = upload_snowml_to_tmp_stage(session, SNOW_ML_WHEEL_LOCAL_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfa9ab88",
   "metadata": {},
   "source": [
    "### Open/Create Model Registry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0a0c8a8",
   "metadata": {},
   "source": [
    "A model registry needs to be created before it can be used. The creation will create a new database in the current account so the active role needs to have permissions to create a database. After the first creation, the model registry can be opened without the need to create it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTRY_DATABASE_NAME = \"TEMP\"\n",
    "REGISTRY_SCHEMA_NAME = \"WZHAO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import model_registry\n",
    "model_registry.create_model_registry(session=session, database_name=REGISTRY_DATABASE_NAME, schema_name=REGISTRY_SCHEMA_NAME)\n",
    "registry = model_registry.ModelRegistry(session=session, database_name=REGISTRY_DATABASE_NAME, schema_name=REGISTRY_SCHEMA_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d76e14a1",
   "metadata": {},
   "source": [
    "## Use with scikit-learn model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c592d46c",
   "metadata": {},
   "source": [
    "### Train A Small Scikit-learn Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "378eb3ba",
   "metadata": {},
   "source": [
    "The cell below trains a small model for demonstration purposes. The nature of the model does not matter, it is purely used to demonstrate the usage of the Model Packaging and Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf44218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "target_digit = 6\n",
    "num_training_examples = 10\n",
    "svc_gamma = 0.001\n",
    "svc_C = 10.0\n",
    "\n",
    "clf = svm.SVC(gamma=svc_gamma, C=svc_C, probability=True)\n",
    "\n",
    "\n",
    "def one_vs_all(dataset, digit):\n",
    "    return [x == digit for x in dataset]\n",
    "\n",
    "\n",
    "# Train a classifier using num_training_examples and use the last 100 examples for test.\n",
    "train_features = digits.data[:num_training_examples]\n",
    "train_labels = one_vs_all(digits.target[:num_training_examples], target_digit)\n",
    "clf.fit(train_features, train_labels)\n",
    "\n",
    "test_features = digits.data[-100:]\n",
    "test_labels = one_vs_all(digits.target[-100:], target_digit)\n",
    "prediction = clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bd0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dda57d0b",
   "metadata": {},
   "source": [
    "SVC has multiple method, for example, `predict_proba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ee333",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_proba = clf.predict_proba(test_features)\n",
    "print(prediction_proba[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "317e7843",
   "metadata": {},
   "source": [
    "### Register Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b482561",
   "metadata": {},
   "source": [
    "The call to `log_model` executes a few steps:\n",
    "1. The given model object is serialized and uploaded to a stage.\n",
    "1. An entry in the Model Registry is created for the model, referencing the model stage location.\n",
    "1. Additional metadata is updated for the model as provided in the call.\n",
    "\n",
    "For the serialization to work, the model object needs to be serializable in python.\n",
    "\n",
    "Aso, you have to provide a sample input data so that we could infer the model signature for you, or you can specify the model signature manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68705420",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_MODEL_NAME=\"SIMPLE_SVC_MODEL\"\n",
    "SVC_MODEL_VERSION=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ad06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A name and model tags can be added to the model at registration time.\n",
    "model_id = registry.log_model(\n",
    "    model_name=SVC_MODEL_NAME,\n",
    "    model_version=SVC_MODEL_VERSION,\n",
    "    model=clf,\n",
    "    tags={\"stage\": \"testing\", \"classifier_type\": \"svm.SVC\", \"svc_gamma\": svc_gamma, \"svc_C\": svc_C},\n",
    "    sample_input_data=test_features[:10],\n",
    ")\n",
    "\n",
    "# The object API can be used to reference a model after creation.\n",
    "model = model_registry.ModelReference(registry=registry, model_name=SVC_MODEL_NAME, model_version=SVC_MODEL_VERSION)\n",
    "print(\"Registered new model:\", model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "735f0ac3",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f778a9ad",
   "metadata": {},
   "source": [
    "We can also restore the model we saved to the registry and load it back into the local context to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "registry = model_registry.ModelRegistry(\n",
    "    session=session, database_name=REGISTRY_DATABASE_NAME, schema_name=REGISTRY_SCHEMA_NAME\n",
    ")\n",
    "model = model_registry.ModelReference(registry=registry, model_name=SVC_MODEL_NAME, model_version=SVC_MODEL_VERSION)\n",
    "restored_clf = model.load_model()\n",
    "\n",
    "restored_prediction = restored_clf.predict(test_features)\n",
    "\n",
    "print(\"Original prediction:\", prediction[:10])\n",
    "print(\"Restored prediction:\", restored_prediction[:10])\n",
    "\n",
    "print(\"Result comparison:\", np.array_equal(prediction, restored_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_prediction_proba = restored_clf.predict_proba(test_features)\n",
    "\n",
    "print(\"Original prediction:\", prediction_proba[:10])\n",
    "print(\"Restored prediction:\", restored_prediction_proba[:10])\n",
    "\n",
    "print(\"Result comparison:\", np.array_equal(prediction_proba, restored_prediction_proba))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45c75e28",
   "metadata": {},
   "source": [
    "### Deploy Model and Batch Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8d496db",
   "metadata": {},
   "source": [
    "We can also deploy the model we saved to the registry to warehouse and predict it in the warehouse.\n",
    "\n",
    "Although the model may contain multiple methods, every deployment can only have one target method, and you need to specify that when you deploy the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c52611ac",
   "metadata": {},
   "source": [
    "Also, since `snowflake-ml-python` does not exist in Anaconda channel yet, we have to import them manually in the options when deploying, it will not required when we our package into Snowflake Anaconda Channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecab97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = model_registry.ModelRegistry(\n",
    "    session=session, database_name=REGISTRY_DATABASE_NAME, schema_name=REGISTRY_SCHEMA_NAME\n",
    ")\n",
    "model = model_registry.ModelReference(registry=registry, model_name=SVC_MODEL_NAME, model_version=SVC_MODEL_VERSION)\n",
    "model.deploy(\n",
    "    deployment_name=\"svc_model_predict\",\n",
    "    target_method=\"predict\",\n",
    "    options={\"_snowml_wheel_path\": SNOW_ML_WHEEL_STAGE_PATH},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e150421",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_prediction = model.predict(deployment_name=\"svc_model_predict\", data=test_features)\n",
    "\n",
    "print(\"Remote prediction:\", remote_prediction[:10])\n",
    "\n",
    "print(\"Result comparison:\", np.array_equal(prediction, remote_prediction[\"feature_0\"].values))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c1f3c07",
   "metadata": {},
   "source": [
    "We can also deploy another method to warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6f189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    deployment_name=\"svc_model_predict_proba\",\n",
    "    target_method=\"predict_proba\",\n",
    "    options={\"_snowml_wheel_path\": SNOW_ML_WHEEL_STAGE_PATH},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a00e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_prediction_proba = model.predict(deployment_name=\"svc_model_predict_proba\", data=test_features)\n",
    "\n",
    "print(\"Remote prediction:\", remote_prediction_proba[:10])\n",
    "\n",
    "print(\"Result comparison:\", np.array_equal(prediction_proba, remote_prediction_proba.values))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc2e2f5e",
   "metadata": {},
   "source": [
    "## Use with customize model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2224cc7",
   "metadata": {},
   "source": [
    "Also with customized model, it could do much more than what shows above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bc58b66",
   "metadata": {},
   "source": [
    "### Download a GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03454cba",
   "metadata": {},
   "source": [
    "### Store GPT-2 Model components locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACTS_DIR = \"/tmp/gpt-2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(os.path.join(ARTIFACTS_DIR, \"model\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(ARTIFACTS_DIR, \"tokenizer\"), exist_ok=True)\n",
    "\n",
    "model.save_pretrained(os.path.join(ARTIFACTS_DIR, \"model\"))\n",
    "tokenizer.save_pretrained(os.path.join(ARTIFACTS_DIR, \"tokenizer\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "333118b7",
   "metadata": {},
   "source": [
    "### Create a custom model using GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c27920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.model import custom_model\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class GPT2Model(custom_model.CustomModel):\n",
    "    def __init__(self, context: custom_model.ModelContext) -> None:\n",
    "        super().__init__(context)\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.context.path(\"model\"))\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.context.path(\"tokenizer\"))\n",
    "\n",
    "    @custom_model.inference_api\n",
    "    def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        def _generate(input_text: str) -> str:\n",
    "            input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "            output = self.model.generate(input_ids, max_length=50, do_sample=True, top_p=0.95, top_k=60)\n",
    "            generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "            return generated_text\n",
    "\n",
    "        res_df = pd.DataFrame({\"output\": pd.Series.apply(X[\"input\"], _generate)})\n",
    "        return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36438fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = GPT2Model(custom_model.ModelContext(models={}, artifacts={\n",
    "    \"model\":os.path.join(ARTIFACTS_DIR, \"model\"),\n",
    "    \"tokenizer\":os.path.join(ARTIFACTS_DIR, \"tokenizer\")\n",
    "}))\n",
    "\n",
    "gpt_model.predict(pd.DataFrame({\"input\":[\"Hello, are you GPT?\"]}))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e111b527",
   "metadata": {},
   "source": [
    "### Register the custom model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c27ed16a",
   "metadata": {},
   "source": [
    "Here, how to specify dependencies and model signature manually is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c22ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_MODEL_NAME = \"GPT2_MODEL\"\n",
    "GPT2_MODEL_VERSION = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a913530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.model import model_signature\n",
    "\n",
    "model_id_gpt = registry.log_model(\n",
    "    model_name=GPT2_MODEL_NAME,\n",
    "    model_version=GPT2_MODEL_VERSION,\n",
    "    model=gpt_model,\n",
    "    conda_dependencies=[\"tensorflow\", \"transformers\"],\n",
    "    signatures={\n",
    "        \"predict\": model_signature.ModelSignature(\n",
    "            inputs=[model_signature.FeatureSpec(name=\"input\", dtype=model_signature.DataType.STRING)],\n",
    "            outputs=[model_signature.FeatureSpec(name=\"output\", dtype=model_signature.DataType.STRING)],\n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "gpt_model = model_registry.ModelReference(registry=registry, model_name=GPT2_MODEL_NAME, model_version=GPT2_MODEL_VERSION)\n",
    "print(\"Registered new model:\", model_id_gpt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e634f4c1",
   "metadata": {},
   "source": [
    "### Deploy the model and predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc0f289d",
   "metadata": {},
   "source": [
    "Relax version is an option that allow the deployer tries to relax the version specifications when initial attempt to\n",
    "resolve the dependencies in Snowflake Anaconda Channel fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d64cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = model_registry.ModelRegistry(\n",
    "    session=session, database_name=REGISTRY_DATABASE_NAME, schema_name=REGISTRY_SCHEMA_NAME\n",
    ")\n",
    "gpt_model = model_registry.ModelReference(\n",
    "    registry=registry,\n",
    "    model_name=GPT2_MODEL_NAME,\n",
    "    model_version=GPT2_MODEL_VERSION,\n",
    ")\n",
    "gpt_model.deploy(\n",
    "    deployment_name=\"gpt_model_predict\",\n",
    "    target_method=\"predict\",\n",
    "    options={\"relax_version\": True, \"_snowml_wheel_path\": SNOW_ML_WHEEL_STAGE_PATH},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24702087",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = gpt_model.predict(deployment_name=\"gpt_model_predict\", data=pd.DataFrame({\"input\":[\"Hello, are you GPT?\"]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b44a55b7",
   "metadata": {},
   "source": [
    "## Use with XGBoost Model, Snowpark DataFrame and permanent deployment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91c61e80",
   "metadata": {},
   "source": [
    "### Prepare a stage for permanent UDF deployment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d420ccdd",
   "metadata": {},
   "source": [
    "A non-temporary and Snowflake internal stage is required to permanently deploy a model as a UDF. We have to create manually now but it will eventually managed by model registry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ccb750",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERMANENT_UDF_STAGE_NAME = \"SNOWML_MODEL_UDF_DEPLOYMENT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b407434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(f\"CREATE OR REPLACE STAGE {PERMANENT_UDF_STAGE_NAME}\").collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3203f803",
   "metadata": {},
   "source": [
    "To make the deployment permanent, any dependency must be put into the a permanent stage as well. Of course, this will no longer be necessary after `snowflake-ml-python` gets available in Snowflake Anaconda channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNOW_ML_WHEEL_STAGE_PATH = upload_snowml_to_tmp_stage(session, SNOW_ML_WHEEL_LOCAL_PATH, f\"@{PERMANENT_UDF_STAGE_NAME}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05e45630",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16debd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_kddcup99\n",
    "\n",
    "DATA_TABLE_NAME = \"KDDCUP99_DATASET\"\n",
    "\n",
    "kddcup99_data = fetch_kddcup99(as_frame=True)\n",
    "kddcup99_sp_df = session.create_dataframe(kddcup99_data.frame)\n",
    "kddcup99_sp_df.write.mode(\"overwrite\").save_as_table(DATA_TABLE_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "771cad94",
   "metadata": {},
   "source": [
    "### Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b976c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.preprocessing import one_hot_encoder, ordinal_encoder, standard_scaler\n",
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "quote_fn = lambda x: f'\"{x}\"'\n",
    "\n",
    "ONE_HOT_ENCODE_COL_NAMES = [\"protocol_type\", \"service\", \"flag\"]\n",
    "ORDINAL_ENCODE_COL_NAMES = [\"labels\"]\n",
    "STANDARD_SCALER_COL_NAMES = [\n",
    "    \"duration\",\n",
    "    \"src_bytes\",\n",
    "    \"dst_bytes\",\n",
    "    \"wrong_fragment\",\n",
    "    \"urgent\",\n",
    "    \"hot\",\n",
    "    \"num_failed_logins\",\n",
    "    \"num_compromised\",\n",
    "    \"num_root\",\n",
    "    \"num_file_creations\",\n",
    "    \"num_shells\",\n",
    "    \"num_access_files\",\n",
    "    \"num_outbound_cmds\",\n",
    "    \"count\",\n",
    "    \"srv_count\",\n",
    "    \"dst_host_count\",\n",
    "    \"dst_host_srv_count\",\n",
    "]\n",
    "\n",
    "TRAIN_SIZE_K = 0.2\n",
    "kddcup99_data = session.table(DATA_TABLE_NAME)\n",
    "kddcup99_data = kddcup99_data.with_columns(\n",
    "    list(map(quote_fn, ONE_HOT_ENCODE_COL_NAMES + ORDINAL_ENCODE_COL_NAMES)),\n",
    "    [\n",
    "        F.to_char(col_name, \"utf-8\")\n",
    "        for col_name in list(map(quote_fn, ONE_HOT_ENCODE_COL_NAMES + ORDINAL_ENCODE_COL_NAMES))\n",
    "    ],\n",
    ")\n",
    "kddcup99_sp_df_train, kddcup99_sp_df_test = tuple(\n",
    "    kddcup99_data.random_split([TRAIN_SIZE_K, 1 - TRAIN_SIZE_K], seed=2568)\n",
    ")\n",
    "\n",
    "ft_one_hot_encoder = one_hot_encoder.OneHotEncoder(\n",
    "    handle_unknown=\"ignore\",\n",
    "    input_cols=list(map(quote_fn, ONE_HOT_ENCODE_COL_NAMES)),\n",
    "    output_cols=ONE_HOT_ENCODE_COL_NAMES,\n",
    "    drop_input_cols=True,\n",
    ")\n",
    "ft_one_hot_encoder = ft_one_hot_encoder.fit(kddcup99_sp_df_train)\n",
    "kddcup99_sp_df_train = ft_one_hot_encoder.transform(kddcup99_sp_df_train)\n",
    "kddcup99_sp_df_test = ft_one_hot_encoder.transform(kddcup99_sp_df_test)\n",
    "\n",
    "ft_ordinal_encoder = ordinal_encoder.OrdinalEncoder(\n",
    "    input_cols=list(map(quote_fn, ORDINAL_ENCODE_COL_NAMES)),\n",
    "    output_cols=list(map(quote_fn, ORDINAL_ENCODE_COL_NAMES)),\n",
    "    drop_input_cols=True,\n",
    ")\n",
    "ft_ordinal_encoder = ft_ordinal_encoder.fit(kddcup99_sp_df_train)\n",
    "kddcup99_sp_df_train = ft_ordinal_encoder.transform(kddcup99_sp_df_train)\n",
    "kddcup99_sp_df_test = ft_ordinal_encoder.transform(kddcup99_sp_df_test)\n",
    "\n",
    "ft_standard_scaler = standard_scaler.StandardScaler(\n",
    "    input_cols=list(map(quote_fn, STANDARD_SCALER_COL_NAMES)),\n",
    "    output_cols=list(map(quote_fn, STANDARD_SCALER_COL_NAMES)),\n",
    "    drop_input_cols=True,\n",
    ")\n",
    "ft_standard_scaler = ft_standard_scaler.fit(kddcup99_sp_df_train)\n",
    "kddcup99_sp_df_train = ft_standard_scaler.transform(kddcup99_sp_df_train)\n",
    "kddcup99_sp_df_test = ft_standard_scaler.transform(kddcup99_sp_df_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4d25ee7",
   "metadata": {},
   "source": [
    "### Train an XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_MODEL_NAME = \"XGB_MODEL_KDDCUP99\"\n",
    "XGB_MODEL_VERSION = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb0f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "regressor = xgboost.XGBClassifier(objective=\"multi:softprob\", n_estimators=500, reg_lambda=1, gamma=0, max_depth=5)\n",
    "kddcup99_pd_df_train = kddcup99_sp_df_train.to_pandas()\n",
    "regressor.fit(\n",
    "    kddcup99_pd_df_train.drop(columns=[\"labels\"]),\n",
    "    kddcup99_pd_df_train[\"labels\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e9446fc",
   "metadata": {},
   "source": [
    "### Log the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf06733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.model import model_signature\n",
    "\n",
    "registry = model_registry.ModelRegistry(\n",
    "    session=session, database_name=REGISTRY_DATABASE_NAME, schema_name=REGISTRY_SCHEMA_NAME\n",
    ")\n",
    "# A name and model tags can be added to the model at registration time.\n",
    "model_id_xgb = registry.log_model(\n",
    "    model_name=XGB_MODEL_NAME,\n",
    "    model_version=XGB_MODEL_VERSION,\n",
    "    model=regressor,\n",
    "    sample_input_data=kddcup99_sp_df_train.drop('\"labels\"'),\n",
    ")\n",
    "\n",
    "# The object API can be used to reference a model after creation.\n",
    "xgb_model = model_registry.ModelReference(registry=registry, model_name=XGB_MODEL_NAME, model_version=XGB_MODEL_VERSION)\n",
    "print(\"Registered new model:\", model_id_xgb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5948b7c8",
   "metadata": {},
   "source": [
    "### Deploy the model permanently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = model_registry.ModelRegistry(\n",
    "    session=session, database_name=REGISTRY_DATABASE_NAME, schema_name=REGISTRY_SCHEMA_NAME\n",
    ")\n",
    "xgb_model = model_registry.ModelReference(\n",
    "    registry=registry,\n",
    "    model_name=XGB_MODEL_NAME,\n",
    "    model_version=XGB_MODEL_VERSION,\n",
    ")\n",
    "xgb_model.deploy(\n",
    "    deployment_name=\"xgb_model_predict\",\n",
    "    target_method=\"predict\",\n",
    "    options={\n",
    "        \"relax_version\": True,\n",
    "        \"permanent_udf_stage_location\": f\"@{PERMANENT_UDF_STAGE_NAME}\",\n",
    "        \"_snowml_wheel_path\": SNOW_ML_WHEEL_STAGE_PATH,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e560bd8d",
   "metadata": {},
   "source": [
    "### Predict with Snowpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9578a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_res = xgb_model.predict(deployment_name=\"xgb_model_predict\", data=kddcup99_sp_df_test)\n",
    "sp_res.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08614b16",
   "metadata": {},
   "source": [
    "### Prepare another SQL connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ff7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "another_session = Session.builder.configs(SnowflakeLoginOptions()).create()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1e99456",
   "metadata": {},
   "source": [
    "### Call the deployed permanent UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8523b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry._session = another_session # Since permanent deployment managing has not been finished in registry.\n",
    "xgb_model = model_registry.ModelReference(\n",
    "    registry=registry,\n",
    "    model_name=XGB_MODEL_NAME,\n",
    "    model_version=XGB_MODEL_VERSION,\n",
    ")\n",
    "sp_res = xgb_model.predict(\n",
    "    deployment_name=\"xgb_model_predict\", data=another_session.create_dataframe(kddcup99_sp_df_test.to_pandas())\n",
    ")\n",
    "sp_res.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b4eabe1",
   "metadata": {},
   "source": [
    "### Remove the deployed UDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be5ecdb5",
   "metadata": {},
   "source": [
    "This would be done by calling delete_deployment in the registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eceb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(f\"DROP FUNCTION xgb_model_predict(object)\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb0a62cbfaa59af7646af5a6672c5c3e72ec75fbadf6ff0336b6769523f221a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
